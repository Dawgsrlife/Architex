"""
Fake LLM Service - Deterministic Mock for Testing

Use this to prove the entire pipeline works WITHOUT burning API quota.
Set FAKE_LLM=true in .env to enable.

This service:
- Returns predictable, deterministic responses
- Generates a realistic file structure
- Completes successfully every time
- Costs zero tokens
"""

import logging
from typing import Dict, Any, Optional, List

from services.llm_interface import LLMInterface

logger = logging.getLogger(__name__)


# Deterministic file set for testing
FAKE_FILES = {
    "README.md": """# {name}

{description}

## Getting Started

```bash
pip install -r requirements.txt
python main.py
```

## Architecture

This project was generated by Architex based on the following prompt:

> {prompt}

## Tech Stack

{tech_stack}
""",
    "main.py": '''"""
{name} - Main Entry Point
Generated by Architex
"""
from fastapi import FastAPI
from pydantic import BaseModel
from typing import List, Optional
from datetime import datetime
import uuid

app = FastAPI(title="{name}", description="{description}")

# In-memory storage
items = {{}}

class Item(BaseModel):
    id: str
    title: str
    description: Optional[str] = None
    status: str = "pending"
    created_at: datetime = None

class ItemCreate(BaseModel):
    title: str
    description: Optional[str] = None

@app.get("/")
def root():
    return {{"message": "Welcome to {name}", "status": "running"}}

@app.get("/items", response_model=List[Item])
def list_items():
    return list(items.values())

@app.post("/items", response_model=Item)
def create_item(item: ItemCreate):
    item_id = str(uuid.uuid4())
    new_item = Item(
        id=item_id,
        title=item.title,
        description=item.description,
        created_at=datetime.utcnow()
    )
    items[item_id] = new_item
    return new_item

@app.get("/items/{{item_id}}", response_model=Item)
def get_item(item_id: str):
    if item_id not in items:
        raise HTTPException(status_code=404, detail="Item not found")
    return items[item_id]

@app.delete("/items/{{item_id}}")
def delete_item(item_id: str):
    if item_id not in items:
        raise HTTPException(status_code=404, detail="Item not found")
    del items[item_id]
    return {{"message": "Item deleted"}}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
''',
    "requirements.txt": """fastapi==0.104.1
uvicorn==0.24.0
pydantic==2.5.2
python-dotenv==1.0.0
""",
    "Dockerfile": """FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

EXPOSE 8000

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
""",
    ".gitignore": """__pycache__/
*.pyc
.env
.venv/
venv/
*.egg-info/
dist/
build/
.pytest_cache/
""",
}


class FakeLLMService(LLMInterface):
    """
    Fake LLM that returns deterministic responses.
    
    Use this to test the full pipeline without API calls.
    """
    
    def __init__(self):
        self._file_queue = []
        self._current_spec = None
        logger.info("FakeLLMService initialized - NO API CALLS WILL BE MADE")
    
    @property
    def is_configured(self) -> bool:
        return True  # Always configured
    
    @property
    def provider_name(self) -> str:
        return "FakeLLM (deterministic mock)"
    
    async def generate_agent_response(
        self,
        history: List[Dict],
        tools: Optional[List[Dict]] = None
    ) -> Dict[str, Any]:
        """
        Return deterministic tool calls based on iteration.
        
        Flow:
        1. First call: write README.md
        2. Second call: write main.py
        3. Third call: write requirements.txt
        4. Fourth call: write Dockerfile
        5. Fifth call: write .gitignore
        6. Sixth call: task_complete
        """
        # Extract spec from first message if available
        if not self._file_queue:
            self._parse_spec_from_history(history)
            self._file_queue = list(FAKE_FILES.keys())
        
        # If we still have files to write
        if self._file_queue:
            filename = self._file_queue.pop(0)
            content = self._format_file_content(filename)
            
            logger.info(f"[FakeLLM] Writing file: {filename}")
            
            return {
                "type": "tool_use",
                "tool": "write_file",
                "params": {
                    "path": filename,
                    "content": content
                }
            }
        
        # All files written - complete the task
        logger.info("[FakeLLM] Task complete")
        return {
            "type": "tool_use",
            "tool": "task_complete",
            "params": {
                "message": "Project generation complete. Created 5 files."
            }
        }
    
    async def generate_architecture(
        self, 
        description: str, 
        requirements: Optional[List] = None,
        tech_stack: Optional[List] = None
    ) -> Dict[str, Any]:
        """Return a fake architecture response."""
        return {
            "architecture": '{"files": {"README.md": "# Fake Project"}}',
            "success": True
        }
    
    def _parse_spec_from_history(self, history: List[Dict]):
        """Extract architecture spec from conversation history."""
        self._current_spec = {
            "name": "Generated Project",
            "description": "A project generated by Architex",
            "prompt": "Build a sample application",
            "tech_stack": ["Python", "FastAPI"]
        }
        
        # Try to extract from first message
        if history and len(history) > 0:
            first_msg = history[0]
            parts = first_msg.get("parts", [])
            if parts:
                content = parts[0] if isinstance(parts[0], str) else str(parts[0])
                
                # Extract name
                if "## PROJECT NAME" in content:
                    lines = content.split("\n")
                    for i, line in enumerate(lines):
                        if "## PROJECT NAME" in line and i + 1 < len(lines):
                            self._current_spec["name"] = lines[i + 1].strip()
                            break
                
                # Extract prompt
                if "## USER REQUEST" in content:
                    lines = content.split("\n")
                    for i, line in enumerate(lines):
                        if "## USER REQUEST" in line and i + 1 < len(lines):
                            self._current_spec["prompt"] = lines[i + 1].strip()
                            break
    
    def _format_file_content(self, filename: str) -> str:
        """Format file content with spec values."""
        template = FAKE_FILES.get(filename, "# Generated by Architex")
        
        spec = self._current_spec or {}
        
        return template.format(
            name=spec.get("name", "Generated Project"),
            description=spec.get("description", "A project generated by Architex"),
            prompt=spec.get("prompt", "Build a sample application"),
            tech_stack="\n".join(f"- {t}" for t in spec.get("tech_stack", ["Python"]))
        )
    
    def reset(self):
        """Reset state for new generation."""
        self._file_queue = []
        self._current_spec = None


# Global instance
fake_llm_service = FakeLLMService()
